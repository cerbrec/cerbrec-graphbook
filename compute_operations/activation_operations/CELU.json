{
  "name": "CELU",
  "primitive_name": "CELU",
  "alises": [],
  "type": "COMPOSITE_OPERATION",
  "description": "Applies the CELU function element wise",
  "input": [
    {
      "name": "alpha",
      "type":"float",
      "default value":1.0,
      "description-of-parameter": "The alpha value for the CELU formulation (see formula section). Determines scaling of the activation for inputs < zero.",
      "assertions": "Input should be a tensor of user's choice, must be a floating point number"
      
    },
    {
      "name": "inplace",
      "type": "bool",
      "default": "False",
      "description": "If True, modifies the input tensor in-place. Default is false."
    }
    
  ],
  "output": [
    {
      "name": "Output 0",
      "primitive_name": "Output",
      "flow_state": "UNBOUND",
      "shape": "shape",
      "value-type": "DECIMAL",
      "description": "A tensor of the same shape as the input, where the CELU activation function has been applied element wise."
    }
    
  ],
  "additional information":[
    {
      "notes": ["CELU is continuously differentiable.", 
      "It is a variant of ELU designed for improved stability in certain optimization scenarios."]

    }
  ],
  "references":[
    {
      "title": "PyTorch Documentation - CELU",
      "url": "https://pytorch.org/docs/stable/generated/torch.nn.CELU.html"
    }
  ]
