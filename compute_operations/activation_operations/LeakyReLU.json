{
"name": "LeakyReLU",
"type": "COMPOSITE_OPERATION",
"description": "Applies the Leaky ReLU function to an input tensor. For each element in the input tensor, if the value is less than 0, it is multiplied by a specified negative slope; otherwise, the value is unchanged.",
"parameters": {
    "inputs": [
        {
            "name": "Input",
            "primitive_name": "Input",
            "type": "Tensor",
            "description": "The input tensor to which the Leaky ReLU function is applied."
        },
        
        {
            "name": "negative_slope",
            "type": "float",
            "description": "The slope of the negative part of the function. Default is 0.01."
        },
        {
            "name":"inplace",
            "type": "bool",
            "description": "Whether to perform the operation in-place (modifying the input tensor directly). Default is False.",
            "optional": true,
            "default": false
        }
    ]
},
    "outputs": [
        {
            "flow_state": "UNBOUND",
            "type": "Tensor",
            "description": "A tensor where each element is the result of applying the Leaky ReLU function with the specified negative slope."
          }
        
        
    ],
    "notes": [
    "Leaky ReLU introduces a small negative slope, which helps to prevent 'dead neurons' where the gradient is zero.",
    "It is commonly used in deep networks to improve learning dynamics."
  ],
  "references": [
    {
      "title": "PyTorch Documentation: leaky_relu",
      "url": "https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.leaky_relu"
    }
  ]
}
