{
        "name": "GLU",
        "type": "COMPOSITE_OPERATION",
        "description": "Applies the Gated Linear Unit (GLU) function to an input tensor. GLU splits the input tensor into two halves along a specified dimension. The first half is multiplied element-wise by the sigmoid activation of the second half, effectively gating the input features.",
        "inputs": [
            {
                "name": "Input",
                "type": "Tensor",
                "description": "The input tensor to which the GLU function is applied. The size of the dimension specified by `dim` must be divisible by 2."
            },
            {
                "name":"dim",
                "type": "int",
                "description": "The dimension along which the input tensor is split into two halves. Default is -1 (the last dimension).",
                "optional": true,
                "default": -1
            }
        ],
        "outputs": [
            {
                "type": "Tensor",
                "description": "A tensor with half the size of the input tensor along the specified dimension, after applying the gating mechanism."
            }
        ],
        "notes": [
        "GLU introduces a gating mechanism that can dynamically control the flow of information through the network.",
        "Ensure the dimension specified by `dim` has an even size; otherwise, an error will occur.",
        "GLU is particularly useful in NLP and other sequence modeling tasks."
      ],
      "references": [
        {
          "title": "PyTorch Documentation: glu",
          "url": "https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.glu"
        },
        {
          "title": "Paper: Language Modeling with Gated Convolutional Networks",
          "url": "https://arxiv.org/abs/1612.08083"
        }
      ]
}
    
