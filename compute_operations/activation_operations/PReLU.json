{
    "name": "PReLU",
    "type": "COMPOSITE_OPERATION",
    "description": "Applies the PReLU function to an input tensor. For each element in the input tensor, if the value is less than 0, it is multiplied by a learnable parameter; otherwise, the value is unchanged.",
    "inputs": [
        {
            "name": "Input",
            "primitive_name": "Input",
            "type": "Tensor",
            "description": "The input tensor to which the PReLU function is applied."
    },
        
        {
            "name": "Weight",
            "primitive_name": "Input (1)",
            "type": "Tensor",
            "description": "A learnable weight tensor that defines the slope of the negative part. This tensor should have the same shape as the number of input channels or the last dimension if the input tensor is multidimensional.",
            "shape": "The shape of weight should match the number of channels or the last dimension of input for non-batched inputs."
        },
        {
            "name":"inplace",
            "type": "bool",
            "description": "Whether to perform the operation in-place (modifying the input tensor directly). Default is False.",
            "optional": true,
            "default": false
        }
    ],
    "outputs": [
        {
            "type": "Tensor",
            "description": "A tensor where each element is the result of applying the PReLU function, with learnable negative slopes."
        }
    ],
    "notes": [
    "PReLU has a learnable parameter that is updated during training, making it more flexible than the standard ReLU.",
    "It is particularly useful in deeper networks where ReLU might suffer from 'dead neurons' (i.e., neurons that stop learning)."
  ],
    "references": [
    {
      "title": "PyTorch Documentation: prelu",
      "url": "https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.prelu"
    }
  ]
}
