{
    "name": "ReLU6",
    "type": "COMPOSITE_OPERATION",
    "description": "Applies the ReLU function with a maximum output value of 6. For each element in the input tensor, if the value is less than 0, it is replaced by 0; if the value is greater than 6, it is replaced by 6. Otherwise, the value is unchanged.",
    "inputs": [
        {
            "name": "Input",
            "primitive_name": "Input",
            "type": "Tensor",
            "description": "The input tensor to which ReLU6 will be applied."
        },
        {
        "name":"inplace",
        "type": "bool",
        "description": "Whether to perform the operation in-place (modifying the input tensor directly). Default is False.",
        "optional": true,
        "default": false
                         }   
        
        
    ],
    "outputs": [
        {
            "name": "Output",
            "primitive_name": "Output",
            
            "type": "Tensor",
            "description": "A tensor where each element is the result of applying ReLU6 to the corresponding element of the input tensor."
            
        }
    ],
    "notes": [
    "ReLU6 is commonly used in models optimized for mobile devices, particularly in quantized neural networks.",
    "It helps to limit the activation values, preventing extreme outputs and aiding numerical stability."
  ],
    "references": [
    {
      "title": "PyTorch Documentation: relu6",
      "url": "https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu6"
    }
  ]
}
