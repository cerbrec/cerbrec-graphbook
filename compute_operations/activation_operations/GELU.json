{
"function": "gelu",
"description": "Applies the Gaussian Error Linear Unit (GELU) activation, which smooths input values using the Gaussian cumulative distribution function.",
  "parameters": {
    "input": {
      "name":"Input",
        "type": "Tensor",
       "description": "The input tensor to which the GELU function is applied."
    },
    "approximate": {
        "name":"approximate",
        "type": "str",
      "description": "If set to 'tanh', uses a faster approximation of the GELU function. Default is 'none', which uses the exact computation.",
      "optional": true,
      "default": "none"
    }
  },
  "returns": {
    "type": "Tensor",
    "description": "A tensor with the same shape as the input tensor, containing the GELU-activated values."
  },
  "example": {
    "code": "import torch\nimport torch.nn.functional as F\n\ninput = torch.tensor([[-1.0, 0.0, 1.0], [2.0, -0.5, 0.5]])\noutput = F.gelu(input)\nprint(output)",
    "output": "tensor([[-0.1588,  0.0000,  0.8413],\n        [ 1.9546, -0.1543,  0.3457]])"
  },
  "notes": [
    "GELU is smoother than ReLU and Leaky ReLU, making it better suited for models where smooth gradients improve performance.",
    "The 'tanh' approximation for GELU is faster and often used in large-scale models.",
    "It is widely used in transformer-based architectures like BERT."
  ],
  "references": [
    {
      "title": "PyTorch Documentation: gelu",
      "url": "https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.gelu"
    },
    {
      "title": "Paper: Gaussian Error Linear Units (GELUs)",
      "url": "https://arxiv.org/abs/1606.08415"
    }
  ]
}
